# Configuraci√≥n de Ollama

Esta gu√≠a te ayudar√° a cambiar de Google Gemini a Ollama (LLM local gratuito).

## ¬øQu√© es Ollama?

Ollama es un servicio local que permite ejecutar modelos de lenguaje grandes (LLMs) en tu propia computadora, sin necesidad de API keys ni l√≠mites de uso. Es completamente gratuito y funciona offline.

## Instalaci√≥n de Ollama

### Windows
1. Descarga el instalador desde: https://ollama.ai/download
2. Ejecuta el instalador
3. Ollama se iniciar√° autom√°ticamente

### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Mac
```bash
brew install ollama
```

## Descargar un Modelo

Una vez instalado Ollama, necesitas descargar un modelo. Abre una terminal y ejecuta:

```bash
# Modelo recomendado (equilibrado entre calidad y velocidad)
ollama pull llama2

# Otras opciones:
ollama pull mistral      # M√°s r√°pido
ollama pull codellama    # Mejor para c√≥digo
ollama pull llama2:13b   # Versi√≥n m√°s grande (mejor calidad, m√°s lento)
```

## Configuraci√≥n en el Proyecto

### Opci√≥n 1: Usar Gemini con Ollama como respaldo (Recomendado)

Esta es la configuraci√≥n m√°s robusta. Gemini ser√° el principal, pero si falla (rate limits, errores), autom√°ticamente usar√° Ollama:

```env
# Usar Gemini como principal
AI_PROVIDER=gemini
GEMINI_API_KEY=tu-api-key-de-gemini

# Activar fallback autom√°tico a Ollama (por defecto: true)
AI_USE_FALLBACK=true

# Configuraci√≥n de Ollama (opcional, estos son los valores por defecto)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```

### Opci√≥n 2: Usar solo Ollama

```env
# Cambiar a Ollama
AI_PROVIDER=ollama

# Configuraci√≥n de Ollama (opcional, estos son los valores por defecto)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Opcional: Si quieres usar Gemini solo para embeddings (mejor calidad)
# GEMINI_API_KEY=tu-api-key-de-gemini
```

### Opci√≥n 3: Usar Ollama con Gemini como respaldo

```env
# Usar Ollama como principal
AI_PROVIDER=ollama
OLLAMA_MODEL=llama2

# Gemini como respaldo
GEMINI_API_KEY=tu-api-key-de-gemini
AI_USE_FALLBACK=true
```

### 2. Verificar que Ollama est√© corriendo

Antes de usar la aplicaci√≥n, aseg√∫rate de que Ollama est√© corriendo:

```bash
# Verificar que Ollama est√© corriendo
curl http://localhost:11434/api/tags
```

Si ves una respuesta JSON, Ollama est√° funcionando correctamente.

## Opciones de Configuraci√≥n

### Usar solo Ollama (completamente gratuito)
```env
AI_PROVIDER=ollama
OLLAMA_MODEL=llama2
```

### Usar Ollama para generaci√≥n + Gemini para embeddings (recomendado)
```env
AI_PROVIDER=ollama
OLLAMA_MODEL=llama2
GEMINI_API_KEY=tu-api-key-de-gemini
```

Esto usa Ollama para generar respuestas (gratis) pero Gemini para embeddings (mejor calidad de b√∫squeda).

### Volver a Gemini
```env
AI_PROVIDER=gemini
GEMINI_API_KEY=tu-api-key-de-gemini
```

## Modelos Disponibles en Ollama

## üéØ ¬øCu√°l Modelo es Mejor para Mesa de Ayuda?

### Recomendaci√≥n Principal: **llama3** o **llama3.2** ‚≠ê

**Para la mayor√≠a de casos de mesa de ayuda, recomiendo:**

```bash
ollama pull llama3
# o la versi√≥n m√°s reciente
ollama pull llama3.2
```

**Configuraci√≥n en `.env`:**
```env
OLLAMA_MODEL=llama3
# o
OLLAMA_MODEL=llama3.2
```

**¬øPor qu√©?**
- ‚úÖ **Excelente calidad** - Entiende contexto y genera respuestas claras
- ‚úÖ **Equilibrado** - Buen balance entre calidad y velocidad
- ‚úÖ **Actualizado** - Versi√≥n m√°s reciente con mejor rendimiento
- ‚úÖ **Requisitos razonables** - ~8GB RAM (m√°s com√∫n)
- ‚úÖ **Ideal para texto** - Perfecto para chat y respuestas de ayuda

### Comparaci√≥n R√°pida

| Modelo | Calidad | Velocidad | RAM | Mejor Para |
|--------|---------|-----------|-----|------------|
| **llama3 / llama3.2** ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | **Mesa de ayuda (RECOMENDADO)** |
| **llama2** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | General, buena opci√≥n |
| **mistral** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | Si necesitas m√°xima velocidad |
| **qwen2.5** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | Alta calidad, multiling√ºe |
| **llama2:13b** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 16GB | Si tienes m√°s RAM y quieres mejor calidad |
| **phi** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 4GB | Si tienes poca RAM |
| **codellama** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 8GB | Solo si necesitas c√≥digo |

### Recomendaciones por Escenario

#### üèÜ **Mejor Opci√≥n General: llama3 o llama3.2**
```bash
ollama pull llama3.2
```
```env
OLLAMA_MODEL=llama3.2
```
**Ideal para:** Mesa de ayuda, chat general, respuestas profesionales

#### ‚ö° **Si Necesitas M√°xima Velocidad: mistral**
```bash
ollama pull mistral
```
```env
OLLAMA_MODEL=mistral
```
**Ideal para:** Respuestas r√°pidas, muchos usuarios simult√°neos

#### üíé **Si Tienes 16GB+ RAM: llama3:8b o llama2:13b**
```bash
ollama pull llama3:8b
# o
ollama pull llama2:13b
```
```env
OLLAMA_MODEL=llama3:8b
```
**Ideal para:** M√°xima calidad, servidores con m√°s recursos

#### üíª **Si Tienes Poca RAM (<8GB): phi o orca-mini**
```bash
ollama pull phi
# o
ollama pull orca-mini
```
```env
OLLAMA_MODEL=phi
```
**Ideal para:** Sistemas con recursos limitados

#### üåç **Si Necesitas Multiling√ºe: qwen2.5**
```bash
ollama pull qwen2.5
```
```env
OLLAMA_MODEL=qwen2.5
```
**Ideal para:** Soporte en m√∫ltiples idiomas

### Modelos Populares Recomendados

#### Modelos Generales (Chat y Texto)
- **llama2** o **llama2:7b**: Modelo equilibrado, recomendado para la mayor√≠a de casos (requiere ~8GB RAM)
- **llama2:13b**: Versi√≥n m√°s grande, mejor calidad pero m√°s lento (requiere ~16GB RAM)
- **llama2:70b**: Versi√≥n muy grande, requiere mucha RAM (requiere ~40GB RAM)
- **mistral** o **mistral:7b**: M√°s r√°pido, buena calidad (requiere ~8GB RAM)
- **mistral:13b**: Versi√≥n m√°s grande de Mistral (requiere ~16GB RAM)
- **gemma:7b**: Modelo de Google, equilibrado y eficiente (requiere ~8GB RAM)
- **phi**: Modelo peque√±o y r√°pido (requiere ~4GB RAM)
- **orca-mini**: Modelo peque√±o, ideal para pruebas (requiere ~4GB RAM)

#### Modelos Especializados en C√≥digo
- **codellama** o **codellama:7b**: Especializado en c√≥digo y programaci√≥n (requiere ~8GB RAM)
- **codellama:13b**: Versi√≥n m√°s grande para c√≥digo complejo (requiere ~16GB RAM)
- **deepseek-coder**: Modelo especializado en c√≥digo (requiere ~8GB RAM)
- **qwen2.5-coder**: Modelo Qwen especializado en c√≥digo (requiere ~8GB RAM)

#### Modelos Multimodales y Avanzados
- **llama3**: Versi√≥n m√°s reciente de Llama (requiere ~8GB RAM)
- **llama3:70b**: Versi√≥n grande de Llama 3 (requiere ~40GB RAM)
- **qwen2.5**: Modelo chino de alta calidad (requiere ~8GB RAM)
- **qwen2.5:14b**: Versi√≥n m√°s grande de Qwen (requiere ~16GB RAM)
- **deepseek-r1**: Modelo avanzado de DeepSeek (requiere ~8GB RAM)

### Ver Modelos Instalados

#### Opci√≥n 1: Desde la Terminal (Recomendado)
```bash
ollama list
```

Esto mostrar√° algo como:
```
NAME            ID              SIZE    MODIFIED
llama2          abc123...       3.8 GB  2 hours ago
mistral         def456...       4.1 GB  1 day ago
codellama       ghi789...       3.8 GB  3 days ago
```

#### Opci√≥n 2: Endpoint de API (Recomendado)
Puedes consultar los modelos desde la API:
```bash
curl http://localhost:8000/ollama-models
```

O desde el navegador:
```
http://localhost:8000/ollama-models
```

Este endpoint te mostrar√°:
- ‚úÖ Si Ollama est√° disponible
- üì¶ Lista de modelos instalados con tama√±o y fecha
- ‚≠ê Modelo actual configurado en `.env`
- ‚ö†Ô∏è Advertencias si el modelo configurado no est√° instalado

La respuesta incluir√°:
```json
{
  "success": true,
  "message": "Modelos obtenidos correctamente",
  "models": [
    {
      "name": "llama2",
      "size": "3.8 GB",
      "modified": "2024-01-15 10:30:00",
      "digest": "abc123..."
    }
  ],
  "current_model": "llama2",
  "ollama_url": "http://localhost:11434"
}
```

### Buscar Modelos Disponibles

Para buscar modelos disponibles en el repositorio de Ollama:
```bash
# Ver modelos populares en el sitio web
# Visita: https://ollama.com/library

# O busca desde la terminal (si tienes acceso)
ollama search [t√©rmino]
```

### Descargar un Modelo

Para descargar un modelo espec√≠fico:
```bash
# Modelos generales
ollama pull llama2
ollama pull mistral
ollama pull gemma:7b
ollama pull llama3

# Modelos de c√≥digo
ollama pull codellama
ollama pull deepseek-coder
ollama pull qwen2.5-coder

# Modelos grandes (requieren m√°s RAM)
ollama pull llama2:13b
ollama pull llama3:70b
ollama pull qwen2.5:14b
```

### Configurar un Modelo Espec√≠fico

Una vez que hayas descargado un modelo, config√∫ralo en tu archivo `.env`:

1. **Abre el archivo `.env`** en la ra√≠z del proyecto

2. **Busca o agrega la l√≠nea `OLLAMA_MODEL`**:
```env
OLLAMA_MODEL=llama2
```

3. **Reemplaza `llama2` con el nombre del modelo que quieres usar**:
```env
# Ejemplos:
OLLAMA_MODEL=qwen2.5-coder    # Para c√≥digo
OLLAMA_MODEL=deepseek-coder   # Para c√≥digo avanzado
OLLAMA_MODEL=mistral          # M√°s r√°pido
OLLAMA_MODEL=llama3           # M√°s reciente
OLLAMA_MODEL=llama2:13b       # Versi√≥n m√°s grande
```

4. **Verifica que el modelo est√© instalado**:
```bash
# Desde la terminal de Ollama
ollama list

# O desde la API
curl http://localhost:8000/ollama-models
```

5. **Reinicia el servidor** si est√° corriendo para que tome los cambios.

**Nota**: El nombre del modelo debe coincidir exactamente con el nombre que muestra `ollama list`. Por ejemplo, si instalaste `qwen2.5-coder`, usa exactamente `qwen2.5-coder` en el `.env`.

### Usar Modelos Personalizados de Ollama

Ollama permite crear modelos personalizados con prompts espec√≠ficos. Esto es √∫til para adaptar el comportamiento del modelo a tu caso de uso (por ejemplo, mesa de ayuda).

#### Crear un Modelo Personalizado

1. **Descarga el modelo base**:
```bash
ollama pull llama3.2
# o cualquier otro modelo base
```

2. **Crea un Modelfile** (archivo de configuraci√≥n):
```bash
# Crea un archivo llamado Modelfile
echo "FROM llama3.2" > Modelfile
echo "SYSTEM Eres un asistente de mesa de ayuda profesional y amigable. Tu objetivo es ayudar a los usuarios con sus consultas de manera clara, concisa y √∫til." >> Modelfile
```

3. **Crea el modelo personalizado**:
```bash
ollama create mi-mesa-ayuda -f Modelfile
```

4. **Config√∫ralo en tu `.env`**:
```env
OLLAMA_MODEL=mi-mesa-ayuda
```

#### Ejemplo: Modelo Personalizado para Mesa de Ayuda

Puedes crear un modelo espec√≠fico para tu aplicaci√≥n de mesa de ayuda:

```bash
# 1. Descargar modelo base
ollama pull llama3.2

# 2. Crear Modelfile personalizado
cat > Modelfile << EOF
FROM llama3.2
SYSTEM Eres un asistente de mesa de ayuda profesional y amigable. 
Tu objetivo es ayudar a los usuarios con sus consultas t√©cnicas y administrativas.
- Responde de manera clara y concisa
- Usa un tono profesional pero amigable
- Proporciona soluciones pr√°cticas
- Si no sabes algo, adm√≠telo y ofrece contactar con soporte t√©cnico
EOF

# 3. Crear el modelo
ollama create mesa-ayuda-asistente -f Modelfile

# 4. Configurar en .env
# OLLAMA_MODEL=mesa-ayuda-asistente
```

#### Usar Modelos de Otros Usuarios (Ollama Hub)

Si alguien ha publicado un modelo en Ollama Hub (como `umarketing343/quokka`), puedes usarlo directamente:

1. **Descargar el modelo**:
```bash
ollama pull umarketing343/quokka
```

2. **Configurarlo en `.env`**:
```env
OLLAMA_MODEL=umarketing343/quokka
```

3. **Verificar que est√© instalado**:
```bash
# Desde la terminal de Ollama
ollama list

# O desde la API
curl http://localhost:8000/ollama-models
```

**Nota**: Los modelos personalizados funcionan igual que los modelos oficiales. Solo necesitas usar el nombre exacto del modelo en tu archivo `.env`.

### Nota sobre los Modelos de la Imagen

Los modelos que aparecen en la imagen (gpt-oss, deepseek-v3.1, qwen3-coder, qwen3-vl, minimax-m2, alm-4.6) son **modelos cloud** de otros proveedores, **NO son modelos de Ollama**.

Ollama tiene sus propios modelos que puedes descargar e instalar localmente. Algunos modelos similares disponibles en Ollama incluyen:
- **qwen2.5-coder** (similar a qwen3-coder)
- **deepseek-coder** (similar a deepseek-v3.1)
- **qwen2.5** (similar a qwen3-vl)

Para ver todos los modelos disponibles en Ollama, visita: https://ollama.com/library

## Sistema de Fallback Autom√°tico

El sistema ahora soporta **fallback autom√°tico** entre Gemini y Ollama:

### ¬øC√≥mo funciona?

1. **Proveedor Principal**: Intenta usar el proveedor configurado (Gemini u Ollama)
2. **Fallback Autom√°tico**: Si el proveedor principal falla (rate limits, errores, etc.), autom√°ticamente intenta con el otro
3. **Transparente**: El usuario no nota la diferencia, solo recibe la respuesta

### Ejemplo de uso:

```env
# Gemini como principal, Ollama como respaldo
AI_PROVIDER=gemini
GEMINI_API_KEY=tu-api-key
AI_USE_FALLBACK=true
OLLAMA_MODEL=llama2
```

**Escenario**: 
- Usuario hace una pregunta ‚Üí Sistema intenta con Gemini
- Gemini responde con error 429 (rate limit) ‚Üí Sistema autom√°ticamente usa Ollama
- Usuario recibe la respuesta sin saber que hubo un cambio

### Ventajas del Fallback

‚úÖ **Alta disponibilidad** - Si un servicio falla, el otro toma el relevo
‚úÖ **Sin interrupciones** - El usuario siempre recibe una respuesta
‚úÖ **Mejor experiencia** - No hay errores por rate limits o problemas temporales
‚úÖ **Flexible** - Puedes desactivar el fallback si quieres: `AI_USE_FALLBACK=false`

## Ventajas de Ollama

‚úÖ **Completamente gratuito** - Sin l√≠mites de uso
‚úÖ **Funciona offline** - No necesitas internet despu√©s de descargar el modelo
‚úÖ **Sin rate limits** - Puedes hacer todas las peticiones que quieras
‚úÖ **Privacidad** - Todo se procesa localmente
‚úÖ **Sin costos** - No hay facturaci√≥n ni API keys

## Desventajas

‚ö†Ô∏è **Requiere recursos** - Necesitas RAM suficiente (m√≠nimo 8GB recomendado)
‚ö†Ô∏è **M√°s lento** - Depende de tu hardware
‚ö†Ô∏è **Embeddings simples** - Ollama no tiene embeddings nativos de alta calidad (puedes usar Gemini solo para embeddings)

## Soluci√≥n de Problemas

### Error: "Ollama no est√° disponible"
- Verifica que Ollama est√© corriendo: `ollama serve`
- Verifica la URL en `.env`: `OLLAMA_BASE_URL=http://localhost:11434`

### Error: "Model not found"
- Descarga el modelo: `ollama pull llama2`
- Verifica el nombre del modelo en `.env`: `OLLAMA_MODEL=llama2`

### Respuestas muy lentas
- Usa un modelo m√°s peque√±o: `ollama pull mistral`
- O reduce el tama√±o del modelo: `OLLAMA_MODEL=llama2:7b`

### Respuestas de baja calidad
- Usa un modelo m√°s grande: `OLLAMA_MODEL=llama2:13b`
- O combina con Gemini para embeddings: configura `GEMINI_API_KEY`

## Notas Importantes

1. **Primera ejecuci√≥n**: La primera vez que uses un modelo, Ollama lo descargar√° (puede tardar varios minutos dependiendo del tama√±o).

2. **Memoria RAM**: Los modelos grandes requieren mucha RAM. Para `llama2:13b` necesitas al menos 16GB de RAM.

3. **Embeddings**: Ollama no tiene embeddings nativos de alta calidad. Si necesitas mejor calidad en la b√∫squeda de documentos, considera usar Gemini solo para embeddings manteniendo Ollama para generaci√≥n.

4. **Rendimiento**: El rendimiento depende de tu hardware. En CPUs modernos, `llama2` puede generar respuestas en 5-15 segundos.

